{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dff1f80-2903-401d-b70a-fa54a779648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy: 1.24.3\n",
      "TensorFlow: 2.13.0\n",
      "Mediapipe: 0.10.21\n",
      "PyAutoGUI: installed\n",
      "OpenCV: 4.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "import cv2\n",
    "\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Mediapipe:\", mp.__version__)\n",
    "print(\"PyAutoGUI: installed\")\n",
    "print(\"OpenCV:\", cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b73ef3-a796-47f5-876b-e22f8c2481d9",
   "metadata": {},
   "source": [
    "# Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec24b608-f1d8-400e-a437-debbee81fbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afd797a-8370-4c32-a1c5-cf28b18b1b0c",
   "metadata": {},
   "source": [
    "# Initialize Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "947b7463-36a4-44da-8c58-b132d2c9d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ff672-4d06-46bf-8426-e60b25870604",
   "metadata": {},
   "source": [
    "# FIST (mute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a12900b-2012-48ce-bf87-c9070fdbcc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 300 frames for Fist. Press 'q' to stop early.\n",
      "Collected 300 frames for Fist gesture.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Gesture name and folder\n",
    "gesture_name = \"Fist\"\n",
    "save_path = f\"gesture_data/{gesture_name}\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "max_frames = 300  # Number of frames to collect\n",
    "\n",
    "print(f\"Collecting {max_frames} frames for {gesture_name}. Press 'q' to stop early.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    # Mirror image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Flatten landmarks: 21 points × 3 (x,y,z) = 63 features\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Save as numpy array\n",
    "            np.save(os.path.join(save_path, f\"{gesture_name}_{frame_count}.npy\"), np.array(landmarks))\n",
    "            frame_count += 1\n",
    "    \n",
    "    # Display count on frame\n",
    "    cv2.putText(frame, f\"{gesture_name} frames: {frame_count}/{max_frames}\", (10,30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    \n",
    "    cv2.imshow(f\"{gesture_name} Capture\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or frame_count >= max_frames:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Collected {frame_count} frames for {gesture_name} gesture.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd39f2a-462b-45cd-bf76-215b7b9b993b",
   "metadata": {},
   "source": [
    "# Gesture 2: OPEN PALM (mouse movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c8495b53-fb26-49b2-8183-06b2dac3753a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting 300 frames for OpenPalm. Press 'q' to stop early.\n",
      "Collected 300 frames for OpenPalm gesture.\n",
      "Collected 19 frames for OpenPalm gesture.\n"
     ]
    }
   ],
   "source": [
    "# Create folder to save Open Palm gesture data\n",
    "gesture_name = \"OpenPalm\"\n",
    "if not os.path.exists(f\"gesture_data/{gesture_name}\"):\n",
    "    os.makedirs(f\"gesture_data/{gesture_name}\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcamimport cv2\n",
    "import os\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Gesture name and folder\n",
    "gesture_name = \"OpenPalm\"\n",
    "save_path = f\"gesture_data/{gesture_name}\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "max_frames = 300  # Number of frames to collect\n",
    "\n",
    "print(f\"Collecting {max_frames} frames for {gesture_name}. Press 'q' to stop early.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    # Mirror image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Flatten landmarks: 21 points × 3 (x,y,z) = 63 features\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            \n",
    "            # Save as numpy array\n",
    "            np.save(os.path.join(save_path, f\"{gesture_name}_{frame_count}.npy\"), np.array(landmarks))\n",
    "            frame_count += 1\n",
    "    \n",
    "    # Display count on frame\n",
    "    cv2.putText(frame, f\"{gesture_name} frames: {frame_count}/{max_frames}\", (10,30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Open Palm Capture\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or frame_count >= max_frames:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Collected {frame_count} frames for {gesture_name} gesture.\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip the frame for mirror view\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_frame)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Save landmarks as numpy array\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append([lm.x, lm.y, lm.z])\n",
    "            landmarks = np.array(landmarks)\n",
    "            np.save(f\"gesture_data/{gesture_name}/{gesture_name}_{frame_count}.npy\", landmarks)\n",
    "            frame_count += 1\n",
    "    \n",
    "    cv2.imshow(\"Open Palm Capture\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or frame_count >= 300:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(f\"Collected {frame_count} frames for {gesture_name} gesture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afa68d-c044-43e3-8579-96e196e2cf47",
   "metadata": {},
   "source": [
    "# working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cbf1f29f-711e-4b30-8856-db2d7e7ecf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pyautogui\n",
    "\n",
    "# Initialize Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "\n",
    "# Example gesture data (simplified for demo)\n",
    "X = [\n",
    "    [0]*42,  # Fist placeholder\n",
    "    [1]*42   # OpenPalm placeholder\n",
    "]\n",
    "y = [\"Fist\", \"OpenPalm\"]\n",
    "\n",
    "# Train KNN\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Flatten landmarks\n",
    "            landmarks = []\n",
    "            for lm in handLms.landmark:\n",
    "                landmarks.extend([lm.x, lm.y])\n",
    "\n",
    "            # Predict gesture\n",
    "            gesture = model.predict([landmarks])[0]\n",
    "            cv2.putText(frame, gesture, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "            # Perform actions\n",
    "            if gesture == \"Fist\":\n",
    "                pyautogui.click()  # Left click\n",
    "            elif gesture == \"OpenPalm\":\n",
    "                # Move mouse based on index finger tip\n",
    "                x = int(handLms.landmark[8].x * screen_width)\n",
    "                y = int(handLms.landmark[8].y * screen_height)\n",
    "                pyautogui.moveTo(x, y)\n",
    "\n",
    "    cv2.imshow(\"Gesture Control\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69dd38-861e-4dc1-bd54-75ce0658f7e4",
   "metadata": {},
   "source": [
    "# Peace sign (scroll down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c3ecdea9-dc00-413f-a82a-c87ee62b60d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peace Sign data collected successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "gesture_data = []  # List to store [landmarks, label]\n",
    "gesture_name = \"Peace\"  # Label for this gesture\n",
    "num_samples = 100  # Number of frames to collect\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "collected = 0\n",
    "\n",
    "while collected < num_samples:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            # Draw landmarks\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Extract landmarks\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append(lm.x)\n",
    "                landmarks.append(lm.y)\n",
    "            gesture_data.append([landmarks, gesture_name])\n",
    "            collected += 1\n",
    "    \n",
    "    cv2.putText(frame, f\"Collected: {collected}/{num_samples}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow(\"Peace Sign Data Collection\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save data\n",
    "with open(\"gesture_data.pkl\", \"ab\") as f:  # append mode to combine with previous gestures\n",
    "    pickle.dump(gesture_data, f)\n",
    "\n",
    "print(\"Peace Sign data collected successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d730ab-2995-49ef-8407-672c277f1833",
   "metadata": {},
   "source": [
    "# train with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90c3d45c-5d28-4ffa-9f7f-03fc83715bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pyautogui\n",
    "\n",
    "# Load all gesture data\n",
    "all_data = []\n",
    "try:\n",
    "    with open(\"gesture_data.pkl\", \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                data = pickle.load(f)\n",
    "                all_data.extend(data)\n",
    "            except EOFError:\n",
    "                break\n",
    "except FileNotFoundError:\n",
    "    print(\"No gesture data found. Collect gestures first.\")\n",
    "    exit()\n",
    "\n",
    "# Prepare features and labels\n",
    "X = [item[0] for item in all_data]  # landmarks\n",
    "y = [item[1] for item in all_data]  # gesture labels\n",
    "\n",
    "# Train KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X, y)\n",
    "print(\"KNN model trained successfully!\")\n",
    "\n",
    "# Initialize Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append(lm.x)\n",
    "                landmarks.append(lm.y)\n",
    "            \n",
    "            prediction = knn.predict([landmarks])[0]  # Predict gesture\n",
    "            cv2.putText(frame, prediction, (10,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "            \n",
    "            # Perform actions\n",
    "            if prediction == \"Peace\":\n",
    "                pyautogui.scroll(-300)  # Scroll down\n",
    "            elif prediction == \"Fist\":\n",
    "                pyautogui.press(\"space\")  # Example: Play/Pause video\n",
    "            elif prediction == \"OpenPalm\":\n",
    "                pyautogui.scroll(300)  # Scroll up\n",
    "\n",
    "    cv2.imshow(\"Gesture Control\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864c2e59-cf35-46e5-84f8-15d00ac1cb74",
   "metadata": {},
   "source": [
    "# Thumbs up (volume up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccc94b4b-51d5-406c-b6c8-c950450cf99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for ThumbsUp. Press 'q' to stop.\n",
      "Data collection for ThumbsUp completed. Total samples: 2129\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "\n",
    "gesture_name = \"ThumbsUp\"\n",
    "data = []\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(f\"Collecting data for {gesture_name}. Press 'q' to stop.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append(lm.x)\n",
    "                landmarks.append(lm.y)\n",
    "            data.append([landmarks, gesture_name])  # Save landmarks with gesture label\n",
    "\n",
    "    cv2.imshow(\"Collect Thumbs Up\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save gesture data\n",
    "try:\n",
    "    with open(\"gesture_data.pkl\", \"rb\") as f:\n",
    "        all_data = pickle.load(f)\n",
    "except:\n",
    "    all_data = []\n",
    "\n",
    "all_data.extend(data)\n",
    "\n",
    "with open(\"gesture_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "print(f\"Data collection for {gesture_name} completed. Total samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de50132-1f66-478f-bc73-df42cce7295a",
   "metadata": {},
   "source": [
    "# Train with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7ebbcb2-e1a4-497d-b7e5-bee22914950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN retrained with new gesture!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pyautogui\n",
    "\n",
    "# Load all gesture data\n",
    "with open(\"gesture_data.pkl\", \"rb\") as f:\n",
    "    all_data = pickle.load(f)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = []\n",
    "y = []\n",
    "for landmarks, label in all_data:\n",
    "    X.append(landmarks)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X, y)\n",
    "print(\"KNN retrained with new gesture!\")\n",
    "\n",
    "# Start webcam for gesture recognition\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(frame_rgb)\n",
    "\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            landmarks = []\n",
    "            for lm in hand_landmarks.landmark:\n",
    "                landmarks.append(lm.x)\n",
    "                landmarks.append(lm.y)\n",
    "            \n",
    "            prediction = knn.predict([landmarks])[0]\n",
    "            cv2.putText(frame, f\"Gesture: {prediction}\", (10, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Map gestures to actions\n",
    "            if prediction == \"ThumbsUp\":\n",
    "                pyautogui.press('volumeup')\n",
    "\n",
    "    cv2.imshow(\"Gesture Control\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d3bdbd-8968-4237-9e6c-a7721d97e472",
   "metadata": {},
   "source": [
    "# Thumbs down (volume down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0442917-5000-477e-a08e-439fd1149d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ThumbsDown data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "gesture_name = \"ThumbsDown\"\n",
    "data = []\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "    \n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        for handLms in result.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "            # Flatten landmarks\n",
    "            landmarks = []\n",
    "            for lm in handLms.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "            data.append(landmarks)\n",
    "    \n",
    "    cv2.putText(frame, f\"Collecting: {gesture_name}\", (10, 30), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow(\"Collecting Thumbs Down\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save data\n",
    "with open(f\"{gesture_name}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(np.array(data), f)\n",
    "\n",
    "print(f\"{gesture_name} data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d44556-a879-448b-acae-3bb16badd711",
   "metadata": {},
   "source": [
    "# All 4 gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e7176495-a888-4e50-a25c-1e7991af266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All landmarks shape: (3024, 63)\n",
      "KNN trained for all gestures!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pyautogui\n",
    "\n",
    "# Load all gesture data\n",
    "all_data = []\n",
    "\n",
    "# Load gesture_data.pkl (ThumbsUp, Fist if saved)\n",
    "try:\n",
    "    with open(\"gesture_data.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        all_data.extend(data)\n",
    "except FileNotFoundError:\n",
    "    print(\"gesture_data.pkl not found, skipping.\")\n",
    "\n",
    "# Load ThumbsDown.pkl\n",
    "try:\n",
    "    with open(\"ThumbsDown.pkl\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        # convert to [landmarks, label]\n",
    "        data = [[item.tolist(), \"ThumbsDown\"] for item in data]\n",
    "        all_data.extend(data)\n",
    "except FileNotFoundError:\n",
    "    print(\"ThumbsDown.pkl not found, skipping.\")\n",
    "\n",
    "# Load OpenPalm .npy files\n",
    "openpalm_folder = \"gesture_data/OpenPalm\"\n",
    "if os.path.exists(openpalm_folder):\n",
    "    for file in os.listdir(openpalm_folder):\n",
    "        if file.endswith(\".npy\"):\n",
    "            landmarks = np.load(os.path.join(openpalm_folder, file)).flatten()\n",
    "            all_data.append([landmarks.tolist(), \"OpenPalm\"])\n",
    "else:\n",
    "    print(\"OpenPalm folder not found, skipping.\")\n",
    "\n",
    "# Prepare features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for landmarks, label in all_data:\n",
    "    # Make sure each sample has 63 features (21 points × 3)\n",
    "    if len(landmarks) == 42:  # Only x,y\n",
    "        landmarks_3d = []\n",
    "        for i in range(21):\n",
    "            x = landmarks[i*2]\n",
    "            y_coord = landmarks[i*2+1]\n",
    "            z = 0\n",
    "            landmarks_3d.extend([x, y_coord, z])\n",
    "        landmarks = landmarks_3d\n",
    "    X.append(landmarks)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"All landmarks shape:\", X.shape)\n",
    "\n",
    "# Train KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X, y)\n",
    "print(\"KNN trained for all gestures!\")\n",
    "\n",
    "# Initialize Mediapipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(max_num_hands=1)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        continue\n",
    "\n",
    "    frame = cv2.flip(frame, 1)  # mirror view\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Flatten landmarks\n",
    "            landmarks = []\n",
    "            for lm in handLms.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z])\n",
    "\n",
    "            # Predict gesture\n",
    "            gesture = knn.predict([landmarks])[0]\n",
    "            cv2.putText(frame, f\"{gesture}\", (10, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            # Map gestures to actions\n",
    "            if gesture == \"OpenPalm\":\n",
    "                # Move mouse using index finger tip\n",
    "                x = int(handLms.landmark[8].x * screen_width)\n",
    "                y = int(handLms.landmark[8].y * screen_height)\n",
    "                pyautogui.moveTo(x, y)\n",
    "            elif gesture == \"Fist\":\n",
    "                pyautogui.press(\"volumemute\")\n",
    "            elif gesture == \"ThumbsUp\":\n",
    "                pyautogui.press(\"volumeup\")\n",
    "            elif gesture == \"ThumbsDown\":\n",
    "                pyautogui.press(\"volumedown\")\n",
    "\n",
    "    cv2.imshow(\"Gesture Control\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fdd74-7c5d-40ba-a65f-18d94ff6e0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (CV_Lab)",
   "language": "python",
   "name": "cv_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
